{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03098cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji_def\n",
    "import Slang\n",
    "import re\n",
    "\n",
    "PATH = r'C:\\Users\\Daniel\\Documents\\NLP Class\\Project\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f932bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in Slang.chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\" and line != 'QPSA?\\tQue Pasa?':\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64bdacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Daniel\\\\anaconda3\\\\nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-776b96ce76d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj_lem\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj_lem\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-776b96ce76d9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj_lem\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj_lem\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body_lower_no_punc_stop_emoj\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-776b96ce76d9>\u001b[0m in \u001b[0;36mlemmatize_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mwordnet_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"N\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"V\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"J\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"R\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mADV\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mpos_tagged_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             )\n\u001b[0;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(PATH + \"train_small_same_date.csv\")\n",
    "valid_data = pd.read_csv(PATH + \"test_small_same_date.csv\")\n",
    "\n",
    "train_data['body'] = train_data['body'].astype(str)\n",
    "valid_data['body'] = valid_data['body'].astype(str)\n",
    "\n",
    "# Preprocessinng guidance from https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "# lowercase\n",
    "train_data[\"body_lower\"] = train_data[\"body\"].str.lower()\n",
    "valid_data[\"body_lower\"] = valid_data[\"body\"].str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "train_data[\"body_lower_no_punc\"] = train_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "valid_data[\"body_lower_no_punc\"] = valid_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop\"] = train_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "valid_data[\"body_lower_no_punc_stop\"] = valid_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "# Convert Emojis\n",
    "# emoji definition source: https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "\n",
    "\n",
    "\n",
    "# def convert_emojis(text):\n",
    "#     for emot in emoji_def.UNICODE_EMO:\n",
    "#         text = re.sub(re.escape(r'('+emot+')'), \"_\".join(emoji_def.UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "#     return text\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "\n",
    "# train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"]\n",
    "# valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"]\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem\"] = train_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem\"] = valid_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# remove URL\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = train_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "\n",
    "# Convert slang to text\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in Slang.chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\" and line != 'QPSA?\\tQue Pasa?':\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = train_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "\n",
    "train_data.to_csv(\"train_small_same_date_clean.csv\")\n",
    "valid_data.to_csv(\"test_small_same_date_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65f9aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(PATH + \"train_big_same_date.csv\")\n",
    "valid_data = pd.read_csv(PATH + \"test_big_same_date.csv\")\n",
    "\n",
    "train_data['body'] = train_data['body'].astype(str)\n",
    "valid_data['body'] = valid_data['body'].astype(str)\n",
    "\n",
    "# Preprocessinng guidance from https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "# lowercase\n",
    "train_data[\"body_lower\"] = train_data[\"body\"].str.lower()\n",
    "valid_data[\"body_lower\"] = valid_data[\"body\"].str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "train_data[\"body_lower_no_punc\"] = train_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "valid_data[\"body_lower_no_punc\"] = valid_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "# Remove stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop\"] = train_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "valid_data[\"body_lower_no_punc_stop\"] = valid_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "# Convert Emojis\n",
    "# emoji definition source: https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "\n",
    "\n",
    "\n",
    "# def convert_emojis(text):\n",
    "#     for emot in emoji_def.UNICODE_EMO:\n",
    "#         text = re.sub(re.escape(r'('+emot+')'), \"_\".join(emoji_def.UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "#     return text\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "\n",
    "# train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"]\n",
    "# valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"]\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem\"] = train_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem\"] = valid_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# remove URL\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = train_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "\n",
    "# Convert slang to text\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in Slang.chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\" and line != 'QPSA?\\tQue Pasa?':\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = train_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "\n",
    "train_data.to_csv(\"train_big_same_date_clean.csv\")\n",
    "valid_data.to_csv(\"test_big_same_date_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45983e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(PATH + \"train_dev_same_date.csv\")\n",
    "valid_data = pd.read_csv(PATH + \"test_dev_same_date.csv\")\n",
    "\n",
    "train_data['body'] = train_data['body'].astype(str)\n",
    "valid_data['body'] = valid_data['body'].astype(str)\n",
    "\n",
    "# Preprocessinng guidance from https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "# lowercase\n",
    "train_data[\"body_lower\"] = train_data[\"body\"].str.lower()\n",
    "valid_data[\"body_lower\"] = valid_data[\"body\"].str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "train_data[\"body_lower_no_punc\"] = train_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "valid_data[\"body_lower_no_punc\"] = valid_data[\"body_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop\"] = train_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "valid_data[\"body_lower_no_punc_stop\"] = valid_data[\"body_lower_no_punc\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "# Convert Emojis\n",
    "# emoji definition source: https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "\n",
    "\n",
    "\n",
    "# def convert_emojis(text):\n",
    "#     for emot in emoji_def.UNICODE_EMO:\n",
    "#         text = re.sub(re.escape(r'('+emot+')'), \"_\".join(emoji_def.UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "#     return text\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"].apply(lambda text: remove_emoji(text))\n",
    "\n",
    "# train_data[\"body_lower_no_punc_stop_emoj\"] = train_data[\"body_lower_no_punc_stop\"]\n",
    "# valid_data[\"body_lower_no_punc_stop_emoj\"] = valid_data[\"body_lower_no_punc_stop\"]\n",
    "\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem\"] = train_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem\"] = valid_data[\"body_lower_no_punc_stop_emoj\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# remove URL\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = train_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem\"].apply(lambda text: remove_urls(text))\n",
    "\n",
    "# Convert slang to text\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "train_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = train_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "valid_data[\"body_lower_no_punc_stop_emoj_lem_url_slang\"] = valid_data[\"body_lower_no_punc_stop_emoj_lem_url\"].apply(lambda text: chat_words_conversion(text))\n",
    "\n",
    "train_data.to_csv(\"train_dev_same_date_clean.csv\")\n",
    "valid_data.to_csv(\"test_dev_same_date_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad4a4ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>kind</th>\n",
       "      <th>parent</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>utc_raw</th>\n",
       "      <th>utc_dt</th>\n",
       "      <th>...</th>\n",
       "      <th>volume</th>\n",
       "      <th>move</th>\n",
       "      <th>month</th>\n",
       "      <th>body_lower</th>\n",
       "      <th>body_lower_no_punc</th>\n",
       "      <th>body_lower_no_punc_stop</th>\n",
       "      <th>body_lower_no_punc_stop_emoj</th>\n",
       "      <th>body_lower_no_punc_stop_emoj_lem</th>\n",
       "      <th>body_lower_no_punc_stop_emoj_lem_url</th>\n",
       "      <th>body_lower_no_punc_stop_emoj_lem_url_slang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145755</td>\n",
       "      <td>166303</td>\n",
       "      <td>He sold for a 10 mil profit already. I like DV...</td>\n",
       "      <td>2021-03-29T19:58:14Z</td>\n",
       "      <td>gsr28j8</td>\n",
       "      <td>t2</td>\n",
       "      <td>t1_gsqpxip</td>\n",
       "      <td>GME</td>\n",
       "      <td>1.617066e+09</td>\n",
       "      <td>2021-03-29 19:58:14+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10075068.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>he sold for a 10 mil profit already. i like dv...</td>\n",
       "      <td>he sold for a 10 mil profit already i like dvf...</td>\n",
       "      <td>sold 10 mil profit already like dvf dont under...</td>\n",
       "      <td>sold 10 mil profit already like dvf dont under...</td>\n",
       "      <td>sell 10 mil profit already like dvf dont under...</td>\n",
       "      <td>sell 10 mil profit already like dvf dont under...</td>\n",
       "      <td>sell 10 mil profit already like dvf dont under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406448</td>\n",
       "      <td>443440</td>\n",
       "      <td>Honestly, this shit got me up earlier, and goi...</td>\n",
       "      <td>2021-04-01T19:25:47Z</td>\n",
       "      <td>gt3bxxc</td>\n",
       "      <td>t2</td>\n",
       "      <td>t3_mi3eli</td>\n",
       "      <td>GME</td>\n",
       "      <td>1.617323e+09</td>\n",
       "      <td>2021-04-01 19:25:47+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>9351417.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>honestly, this shit got me up earlier, and goi...</td>\n",
       "      <td>honestly this shit got me up earlier and going...</td>\n",
       "      <td>honestly shit got earlier going bed earlier pu...</td>\n",
       "      <td>honestly shit got earlier going bed earlier pu...</td>\n",
       "      <td>honestly shit get earlier go bed earlier put u...</td>\n",
       "      <td>honestly shit get earlier go bed earlier put u...</td>\n",
       "      <td>honestly shit get earlier go bed earlier put u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226054</td>\n",
       "      <td>673216</td>\n",
       "      <td>I appreciate the confidence. I will try this w...</td>\n",
       "      <td>2021-03-11T19:02:09Z</td>\n",
       "      <td>gqn11oa</td>\n",
       "      <td>t2</td>\n",
       "      <td>t1_gqmz84n</td>\n",
       "      <td>GME</td>\n",
       "      <td>1.615511e+09</td>\n",
       "      <td>2021-03-11 19:02:09+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>28402777.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>i appreciate the confidence. i will try this w...</td>\n",
       "      <td>i appreciate the confidence i will try this wh...</td>\n",
       "      <td>appreciate confidence try next see</td>\n",
       "      <td>appreciate confidence try next see</td>\n",
       "      <td>appreciate confidence try next see</td>\n",
       "      <td>appreciate confidence try next see</td>\n",
       "      <td>appreciate confidence try next see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319915</td>\n",
       "      <td>1091065</td>\n",
       "      <td>Insider knowledge. Shill idiot doesn't realize...</td>\n",
       "      <td>2021-03-03T17:12:10Z</td>\n",
       "      <td>gplau59</td>\n",
       "      <td>t2</td>\n",
       "      <td>t3_lwuqgq</td>\n",
       "      <td>GME</td>\n",
       "      <td>1.614813e+09</td>\n",
       "      <td>2021-03-03 17:12:10+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>19325198.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>insider knowledge. shill idiot doesn't realize...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize t...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize c...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize c...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize c...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize c...</td>\n",
       "      <td>insider knowledge shill idiot doesnt realize c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400977</td>\n",
       "      <td>259725</td>\n",
       "      <td>Mods r fuk 🌈🐻</td>\n",
       "      <td>2021-04-05T17:06:11Z</td>\n",
       "      <td>gti44np</td>\n",
       "      <td>t2</td>\n",
       "      <td>t3_mkizw4</td>\n",
       "      <td>GME</td>\n",
       "      <td>1.617660e+09</td>\n",
       "      <td>2021-04-05 17:06:11+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>14223605.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>mods r fuk 🌈🐻</td>\n",
       "      <td>mods r fuk 🌈🐻</td>\n",
       "      <td>mods r fuk 🌈🐻</td>\n",
       "      <td>mods r fuk</td>\n",
       "      <td>mod r fuk</td>\n",
       "      <td>mod r fuk</td>\n",
       "      <td>mod r fuk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0_x  \\\n",
       "0      145755        166303   \n",
       "1      406448        443440   \n",
       "2      226054        673216   \n",
       "3      319915       1091065   \n",
       "4      400977        259725   \n",
       "\n",
       "                                                body           created_utc  \\\n",
       "0  He sold for a 10 mil profit already. I like DV...  2021-03-29T19:58:14Z   \n",
       "1  Honestly, this shit got me up earlier, and goi...  2021-04-01T19:25:47Z   \n",
       "2  I appreciate the confidence. I will try this w...  2021-03-11T19:02:09Z   \n",
       "3  Insider knowledge. Shill idiot doesn't realize...  2021-03-03T17:12:10Z   \n",
       "4                                      Mods r fuk 🌈🐻  2021-04-05T17:06:11Z   \n",
       "\n",
       "        id kind      parent subreddit       utc_raw  \\\n",
       "0  gsr28j8   t2  t1_gsqpxip       GME  1.617066e+09   \n",
       "1  gt3bxxc   t2   t3_mi3eli       GME  1.617323e+09   \n",
       "2  gqn11oa   t2  t1_gqmz84n       GME  1.615511e+09   \n",
       "3  gplau59   t2   t3_lwuqgq       GME  1.614813e+09   \n",
       "4  gti44np   t2   t3_mkizw4       GME  1.617660e+09   \n",
       "\n",
       "                      utc_dt  ...      volume move month  \\\n",
       "0  2021-03-29 19:58:14+00:00  ...  10075068.0    1     3   \n",
       "1  2021-04-01 19:25:47+00:00  ...   9351417.0   -1     4   \n",
       "2  2021-03-11 19:02:09+00:00  ...  28402777.0    1     3   \n",
       "3  2021-03-03 17:12:10+00:00  ...  19325198.0    1     3   \n",
       "4  2021-04-05 17:06:11+00:00  ...  14223605.0    1     4   \n",
       "\n",
       "                                          body_lower  \\\n",
       "0  he sold for a 10 mil profit already. i like dv...   \n",
       "1  honestly, this shit got me up earlier, and goi...   \n",
       "2  i appreciate the confidence. i will try this w...   \n",
       "3  insider knowledge. shill idiot doesn't realize...   \n",
       "4                                      mods r fuk 🌈🐻   \n",
       "\n",
       "                                  body_lower_no_punc  \\\n",
       "0  he sold for a 10 mil profit already i like dvf...   \n",
       "1  honestly this shit got me up earlier and going...   \n",
       "2  i appreciate the confidence i will try this wh...   \n",
       "3  insider knowledge shill idiot doesnt realize t...   \n",
       "4                                      mods r fuk 🌈🐻   \n",
       "\n",
       "                             body_lower_no_punc_stop  \\\n",
       "0  sold 10 mil profit already like dvf dont under...   \n",
       "1  honestly shit got earlier going bed earlier pu...   \n",
       "2                 appreciate confidence try next see   \n",
       "3  insider knowledge shill idiot doesnt realize c...   \n",
       "4                                      mods r fuk 🌈🐻   \n",
       "\n",
       "                        body_lower_no_punc_stop_emoj  \\\n",
       "0  sold 10 mil profit already like dvf dont under...   \n",
       "1  honestly shit got earlier going bed earlier pu...   \n",
       "2                 appreciate confidence try next see   \n",
       "3  insider knowledge shill idiot doesnt realize c...   \n",
       "4                                        mods r fuk    \n",
       "\n",
       "                    body_lower_no_punc_stop_emoj_lem  \\\n",
       "0  sell 10 mil profit already like dvf dont under...   \n",
       "1  honestly shit get earlier go bed earlier put u...   \n",
       "2                 appreciate confidence try next see   \n",
       "3  insider knowledge shill idiot doesnt realize c...   \n",
       "4                                          mod r fuk   \n",
       "\n",
       "                body_lower_no_punc_stop_emoj_lem_url  \\\n",
       "0  sell 10 mil profit already like dvf dont under...   \n",
       "1  honestly shit get earlier go bed earlier put u...   \n",
       "2                 appreciate confidence try next see   \n",
       "3  insider knowledge shill idiot doesnt realize c...   \n",
       "4                                          mod r fuk   \n",
       "\n",
       "          body_lower_no_punc_stop_emoj_lem_url_slang  \n",
       "0  sell 10 mil profit already like dvf dont under...  \n",
       "1  honestly shit get earlier go bed earlier put u...  \n",
       "2                 appreciate confidence try next see  \n",
       "3  insider knowledge shill idiot doesnt realize c...  \n",
       "4                                          mod r fuk  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
